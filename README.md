# LyricSimilarity
I intend to focus my final project on a dataset of the top 10 songs in the US from 1950 to 2019 found on Kaggle here: https://www.kaggle.com/datasets/stefancomanita/top-us-songs-from-1950-to-2019-w-lyrics. The source is poorly documented, but the classification of “top 10” matters less to this project than actual lyrics, as I want to purely focus on lyrics (as this is an English class). If I want more data, I plan to include this dataset: https://www.kaggle.com/datasets/karnikakapoor/lyrics. 

Stakes-wise, I think that music is really good at reflecting cultural trends, while also being difficult to quantify, so I’m hoping that these datasets end up being something fun and interesting to work with. If it works, this project will let me gain a unique look into how popular songs are lyrically similar and show if they can be grouped into distinct categories. 

My current plan is to use Python’s pandas and NLTK packages to take the CSV file and process each song’s lyrics into individual word tokens, excluding stopwords. I can then count unique words across this corpus and take the top n most frequently used unique words. Then each song’s lyrics can be expressed as an n-dimensional vector where each feature k represents how often the song uses the k-th most common word of the corpus.

Then (after normalization) I can use some loss function like mean squared error to compare each song to the others and find which are most lyrically similar. I can also use principal component analysis to create a visualization of song similarity. I’ll have to select a library to do this once I get there, and will probably gain an outside source here from the reading I need to do to implement this well. 

My biggest worry is that songs are not long enough to have many same-word similarities with each other, and so most of the lyrical information will be lost when the song is compressed into its vector form. This will mean that the rest of the analysis will essentially be working with random noise and not reflect anything important. I’ll have to look at the initial unique word count data of the whole corpus to determine if this will become a problem. 
